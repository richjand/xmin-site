---
title: Measuring Run/Pass Tendencies with tidyModels and nflfastR
author: Richard Anderson
date: '2020-09-02'
slug: measuring-run-pass-tendencies-with-tidymodels-and-nflfastr
categories: []
tags:
  - R
  - stan
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>As a Seahawks fan who spends a good amount of time online I’ve been exposed to a lot of discussion about the value of running vs. passing. A point used in favor of a rushing-focused attack is that good teams tend to run the ball a lot. This is usually met with the response that teams who are ahead will run to kill clock and minimize risk, meaning the causal arrow is backwards. The Patriots always run a lot, but the Patriots are always ahead so of course they are going to run.</p>
<p>A common strategy to establish whether a team is run/pass heavy is to identify a subset of plays where the team is not bound by needing to pass to catch up or run to kill clock (<a href="https://twitter.com/SharpFootball/status/1281657545598341121">See Warren Sharp’s Tweet</a> and see what decisions teams actually made. If we see that the Patriots pass while games are competitive and run when they are closing out the game then we know that the Pats winning isn’t caused by rushing. The problem with this approach is that it tends to throw away a lot of useful information. Seeing a team run on 2nd and 11 (again, Seahawks fan here) tells us something very different than seeing a team run on 2nd and 1 just as throwing on 3rd and 1 tells us something different than throwing on 3rd and 10. Thanks to the awesome people at nflscrapR and nflfastR we can build that kind of context into our analysis.</p>
<p>All of the code that you’ll see below can be found at <a href="https://github.com/richjand/nfl-run-pass">My GitHub</a>.</p>
<div id="estimating-prqb-dropback" class="section level2">
<h2>Estimating Pr(QB Dropback)</h2>
<p>The thing we ultimately want to understand is team tendencies. Once we account for the state of the game and any other information of interest, does it seem that team x is more run-focused or pass-focused? This post is basically an exercise in feature engineering where we’re trying to create a measure (dropback probability) that we can use as an input in another model that we’ll use to understand team tendencies. The model we want to build to is:</p>
<center>
<p><span class="math inline">\(y_{it} \sim Bernoulli(p_{it})\)</span></p>
<p><span class="math inline">\(logit(p_{it}) = \alpha + \gamma_{t} + \beta_{1}\hat{p}_i + \boldsymbol\beta\textbf{X}_{i}\)</span></p>
</center>
<p>where <span class="math inline">\(y_{it}\)</span> is going to be whether team <span class="math inline">\(t\)</span> called a pass on play <span class="math inline">\(i\)</span>, <span class="math inline">\(\gamma_{t}\)</span> is a team effect which will be our measure of team strategy, and <span class="math inline">\(\boldsymbol\beta\textbf{X}_{i}\)</span> is going to be any other information we want to include such as quarterback ability, quality of defense, weather, or anything else of interest. <span class="math inline">\(\hat{p}_i\)</span> is the probability of a QB dropback that we’ll generate with our model below. In effect, this will give us an expectation from which we’ll measure deviances at the team level.</p>
<p>Part of the impetus for this project was to learn how to use the <a href="https://www.tidymodels.org/">tidyModels</a> and <a href="https://github.com/tidymodels/parsnip">parsnip</a> packages so I’m going to cover how I built the models with these packages in some detail. I won’t be offended if you want to skip ahead. Also, before getting into the details I’ll link interested readers to posts by <a href="https://juliasilge.com/blog/xgboost-tune-volleyball/">Julia Silge</a> and <a href="http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/">Rebecca Barter</a> which were extremely helpful in getting up and running in the tidyModels universe. I really can’t overemphasize how good these two posts were.</p>
<p>I predict the probability of a QB dropback using the nflfastR-provided variables that collectively capture the game state at the time of the play. These variables aren’t an exhaustive list of what goes into the game state, but hopefully capture most of the information relevant to teams in making the decision to run or pass. The variables are:</p>
<ul>
<li>Down (limited to 1,2,3)</li>
<li>Yards for first down</li>
<li>Yard line</li>
<li>Score Differential</li>
<li>Quarter</li>
<li>Time remaining in half</li>
<li>Number of timeouts for the offense and defense</li>
</ul>
<p>Note that a QB dropback is not the same as saying a pass occurred. QB dropbacks are plays where the offense intended to pass, even if they did not end up in an attempted pass (sacks, scrambles, etc…).</p>
<p>I’m going to use an xgboost model because we know there are non-linearities in the relationship between independent variables and dependent variable as well as some complex interactions between the variables. I can’t say anything about xgboost that hasn’t been said better in a million other data science posts so I’ll just say that I, like so many others, have found xgboost extremely useful for a variety of projects. That said, I’ve found the xgboost package to be a little difficult to work with which is why I’m excited to give tidyModels a try.</p>
</div>
<div id="tidying-my-xgboost" class="section level2">
<h2>Tidying my xgboost</h2>
<div id="prepping-the-data" class="section level3">
<h3>Prepping the Data</h3>
<p>The first step is going to be to split my data into train and test, which we can do with the <code>initial_split</code> function. By default this function will use 75% of the data for training and the remaining 25% for testing. I’ve limited the data to 2016-2019, which leaves ~100k observations for training and ~35k observations for testing.</p>
<pre class="r"><code>dat_split &lt;- initial_split(post16)
dat_train &lt;- training(dat_split)
dat_test &lt;- testing(dat_split)</code></pre>
<p>We’re going be tuning our xgboost hyperparameters so we’ll want to cross-validate to see which hyperparameters give us the best performance. We can create cross-validation sets using <code>vfold_cv()</code>.</p>
<pre class="r"><code>qb_folds &lt;- vfold_cv(dat_train)</code></pre>
</div>
<div id="prepping-the-model" class="section level3">
<h3>Prepping the Model</h3>
<p>Next we’ll define a recipe using the <code>recipe()</code> function from the <code>recipes</code> package. Recipes involve setting a formula that looks like what you use to train most models in R and doing any pre-processing (scaling, normalizing, etc…) that you want to do to your variables. The nice thing about the recipe formulation is that it is the same regardless of which model you’ll ultimately be using so you don’t need to remember how data needs to be fed into <code>glmnet</code> vs. <code>xgboost</code> vs. <code>glm</code>. Since I’m working with xgboost I don’t need to worry about normalizing or regularizing my variables so I’ll just define my formula below.</p>
<pre class="r"><code>qb_recipe &lt;- recipe(qb_dropback ~ down + 
                      ydstogo + 
                      yardline_100 + 
                      score_differential + 
                      qtr + 
                      half_seconds_remaining +
                      off_to +
                      def_to,
    data = dat_train)</code></pre>
<p>Now that we have a recipe we will get our model set up. We’re going to use a boosted tree model which carries with it a bunch of tuneable hyperparameters. I’m going to fix the number of trees to keep cross-validation from getting out of hand and tell the model to stop when there has been no improvement in 100 rounds. Everything else is going to be selected based on model fit.</p>
<p>The <code>set_engine()</code> specifies the package that the model is coming from so if you preferred to use <code>gbm</code> instead of <code>xgboost</code> you would specify <code>set_engine(“gbm”)</code>.</p>
<pre class="r"><code>qb_model &lt;- 
  boost_tree(
    mtry = tune(),
    trees = 2000, 
    min_n = tune(),
    tree_depth = tune(),
    learn_rate = tune(),
    loss_reduction = tune(),                    
    sample_size = tune(),         
    stop_iter = 100
  ) %&gt;% 
  set_engine(&quot;xgboost&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)</code></pre>
<p>Finally, we’re going to specify a workflow which is going to gather the recipe and model we built above. This is going to make it very easy to do parameter tuning and model building without repeatedly specifying the same information.</p>
<pre class="r"><code>qb_workflow &lt;- workflow() %&gt;%
  add_recipe(qb_recipe) %&gt;%
  add_model(qb_model)</code></pre>
</div>
<div id="parameter-tuning" class="section level3">
<h3>Parameter Tuning</h3>
<p>Now it’s time to actually do some modeling! We’ll use our cross-validation folds to try a bunch of different potential parameter values and return which gives us the best out of sample fit. We’ll try 40 different combinations sampled from across the hyperparameter space. Note that the <code>mtry</code> and <code>sample_size</code> parameters require additional arguments. <code>mtry()</code> refers to the number of columns to be sampled at each split. This is one where you need to be careful. If the data frame you specify for <code>finizalize</code> has more variables than you actually plan on training with, you will waste your time testing mtry values that don’t make any sense for your problem. The <code>sample_size</code> argument requires a number between 0 and 1 as it’s the proportion of the data that you’ll use in the fitting routine.</p>
<pre class="r"><code>xgb_grid &lt;- grid_latin_hypercube(
  finalize(mtry(), dat_train),
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  sample_size = sample_prop(),
  size = 40
)</code></pre>
<p>Tuning your grid is as easy as specifying a workflow, your cross-validation data, and the grid of values to be tested. I specify <code>save_pred = TRUE</code> which is going to save all of the cross-validation predictions for later evaluation. Note that this is going to take awhile. A grid of 40 took ~6 hours on my machine. I’d set this off overnight and save the results so you can reload the object without rebuilding every time.</p>
<pre class="r"><code>  xgb_res &lt;- tune_grid(
    qb_workflow,
    resamples = qb_folds,
    grid = xgb_grid,
    control = control_grid(save_pred = TRUE)
  )</code></pre>
<p>Julia Silge’s post has a plot to show the relationship between different parameter values and model performance that I’m going to use here. On the y-axis we have the AUC of the model and on the x-axis we have the value of the hyperparameter. We’re looking to see if there are any obvious correlations between performance and hyperparameter value and if we might need to expand the range of tested values. It’s tough to draw any sweeping conclusions though it looks like higher values of mtry and, to a certain extent, tree depth perform better. It also doesn’t appear that the best values of our hyperparameters are on the edges of our plots. Were it the case that performance was clearly increasing with higher tree depth and we didn’t see a point at which model performance began to decline we would want to extend the range of hyperparameters that we test to make sure that we aren’t setting those values too low.</p>
<pre class="r"><code>xgb_res %&gt;%
  collect_metrics() %&gt;%
  filter(.metric == &quot;roc_auc&quot;) %&gt;%
  dplyr::select(mean, mtry:sample_size) %&gt;%
  pivot_longer(mtry:sample_size,
               values_to = &quot;value&quot;,
               names_to = &quot;parameter&quot;
  ) %&gt;%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = &quot;free_x&quot;) +
  labs(x = NULL, y = &quot;AUC&quot;) +
  theme_minimal()</code></pre>
<p><img src="/post/2020-09-02-measuring-run-pass-tendencies-with-tidymodels-and-nflfastr_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>We can extract the best-performing set of hyperparameters using the <code>select_best()</code> function and use those values to finalize our workflow.</p>
<pre class="r"><code>best_auc &lt;- select_best(xgb_res, &quot;roc_auc&quot;)

qb_xgb &lt;- finalize_workflow(
  qb_workflow,
  parameters = best_auc
)</code></pre>
<p>At this point we’re ready to evaluate the performance of the model trained on our training data with our chosen hyperparameters on our test data which we can do with the <code>last_fit()</code> function. We’ll need to give the function our finalized workflow as well as our split data.</p>
<pre class="r"><code>final_mod &lt;- last_fit(qb_xgb, dat_split)</code></pre>
</div>
<div id="model-evaluation" class="section level3">
<h3>Model Evaluation</h3>
<p>We can find out just how well the model did using <code>collect_metrics()</code>. We ended up with 69% accuracy and an AUC of .76 which seems about right given the application. If we could perfectly predict dropback probability from game state it would be very easy to be an NFL defensive coordinator. Again, Julia Silge did a great job visualizing model outputs in her post and I’m going to basically lift her code for this ROC curve plot</p>
<pre class="r"><code>collect_metrics(final_mod)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.691
## 2 roc_auc  binary         0.760</code></pre>
<pre class="r"><code>final_mod %&gt;%
  collect_predictions() %&gt;%
  roc_curve(qb_dropback, .pred_0) %&gt;%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = &quot;midnightblue&quot;) +
  xlab(&#39;1 - Specificity&#39;) +
  ylab(&#39;Sensitivity&#39;) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = &quot;gray50&quot;,
    size = 1.2
  ) +
  ggtitle(&#39;ROC Curve&#39;) +
  theme_minimal()</code></pre>
<p><img src="/post/2020-09-02-measuring-run-pass-tendencies-with-tidymodels-and-nflfastr_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>As a final check on our results let’s look at calibration in our test data. We want our predicted dropback probabilities to be similar to the actual dropback probabilities and it looks like that’s the case! There’s only 14 plays in the far right dot so I’m not going to lose any sleep over it.</p>
<pre class="r"><code>final_mod %&gt;%
  collect_predictions() %&gt;%
  mutate(pred_rounded = round(.pred_1,1)) %&gt;%
  group_by(pred_rounded) %&gt;%
  summarise(mean_prediction = mean(.pred_1),
            mean_actual = mean(as.numeric(qb_dropback) - 1),
            n = n(),
            se = sd(as.numeric(qb_dropback) - 1 - .pred_1)/sqrt(n)) %&gt;%
  ggplot(aes(x = pred_rounded, y = mean_actual)) +
  geom_abline() +
  geom_point(aes(size = n)) +
  theme_minimal() +
  xlab(&#39;Predicted Probability&#39;) +
  ylab(&#39;Actual Probability&#39;) +
  ggtitle(&#39;Calibration Plot, Test Data&#39;) +
  ylim(0,1) +
  xlim(0,1)</code></pre>
<p><img src="/post/2020-09-02-measuring-run-pass-tendencies-with-tidymodels-and-nflfastr_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Finally, now that we’ve built some confidence in the model we’re going to build (using <code>fit()</code>) and predict (using <code>predict()</code>) the model on all data since 2016.</p>
<pre class="r"><code>final_qb_mod &lt;- fit(qb_xgb, post16)</code></pre>
<pre class="r"><code>post16_pred_dat &lt;- filter(dat, season &gt;= 2016 &amp; 
                   season_type == &#39;REG&#39; &amp; 
                   down %in% c(1,2,3) &amp;
                   !is.na(qb_dropback) &amp;
                   !is.na(score_differential)) %&gt;%
  mutate(qb_dropback = factor(qb_dropback),
         off_to = if_else(posteam_type == &#39;away&#39;, away_timeouts_remaining, home_timeouts_remaining),
         def_to = if_else(posteam_type == &#39;away&#39;, home_timeouts_remaining, away_timeouts_remaining)) %&gt;%
  dplyr::select(qb_dropback, down, ydstogo, yardline_100, score_differential, qtr, half_seconds_remaining, off_to, def_to, epa, posteam, defteam, season)

post16_pred_dat$dropback_prob &lt;- predict(final_qb_mod, new_data = post16_pred_dat, type = &#39;prob&#39;)$.pred_1</code></pre>
<p>As a basic sanity check let’s make sure the model thinks passing is more likely in situations that we would expect. Generally speaking, throwing is more likely on third down and more likely with more yards to go which is what we’d hope to see.</p>
<p><img src="/post/2020-09-02-measuring-run-pass-tendencies-with-tidymodels-and-nflfastr_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
</div>
<div id="a-quick-look-at-team-tendencies" class="section level2">
<h2>A Quick Look At Team Tendencies</h2>
<p>In the future we’ll want to build a model that builds in additional information, but for now we can build a simple model to get an idea of which teams were more or less likely to pass than we would expect based on game script alone. Going back to the equation at the top of the post, we’ll fit a multilevel model where we predict the probability of a QB dropback as a function of our predicted dropback probability along with team random effects. We can interpret these effects as the degree to which teams differ from the expectation set out by the model we made above.</p>
<p>We’ll fit the model in stan, a popular language for fitting Bayesian models and one that people find especially useful for multilevel models. The stan code and the code to build the model in R is displayed below.</p>
<pre class="r"><code>data{
  int&lt;lower = 0&gt; N; //number of observations
  int&lt;lower = 1&gt; I; //number of team/seasons
  int&lt;lower = 0, upper = 1&gt; y[N]; //qb_dropback
  int&lt;lower = 0, upper = I&gt; ii[N]; //team/season indicator
  vector[N] phat; //fitted probability from xgboost model
}
parameters{
  vector[I] mu_raw; //team/season random effects
  real beta_phat; //effect of p_hat, should be ~ 1
  real alpha; //intercept
  real&lt;lower = 0&gt; sigma_mu; //standard deviation of random effects
}
transformed parameters{
  vector[I] mu = sigma_mu * mu_raw;
}
model{
  alpha ~ normal(0, .25);
  beta_phat ~ normal(1,.25);
  mu_raw ~ normal(0,1);
  sigma_mu ~ normal(0,1);
  
  y ~ bernoulli_logit(alpha + mu[ii] + beta_phat * phat);
}</code></pre>
<pre class="r"><code>stan_mod &lt;- stan_model(file = &#39;/stan-models/pass-prob-stan-model-no-epa.stan&#39;)

stan_dat_no_epa &lt;- list(
  N = nrow(final_pred_dat),
  I = max(final_pred_dat$team_idx),
  y = as.numeric(final_pred_dat$qb_dropback) - 1,
  ii = final_pred_dat$team_idx,
  phat = arm::logit(final_pred_dat$dropback_prob)
)

fit_no_epa &lt;- sampling(stan_mod, data = stan_dat_no_epa, cores = 4, chains = 4, iter = 1000)</code></pre>
<p>Below we’ll print some parameters from the model. <code>alpha</code> is the intercept, <code>beta_phat</code> is the coefficient on the predicted pass probability from our xgboost model, and <code>sigma_mu</code> is the standard deviation in team effects. We’d expect a coefficient of 1 on <code>beta_phat</code>, so I should probably go back and look at why it’s coming out a little high. While there’s clearly a difference between beta_phat and our expectation, it’s pretty small in substantive terms. If our xgboost model was saying that the probability of a pass is .6, this model would suggest that that true probability is something like .61 for an average team. The .18 value of <code>sigma_mu</code> means that our predicted probabilities for different teams would range from about .52 on the low end and .69 on the high end for a play where an average team is at .6.</p>
<pre class="r"><code>print(fit_no_epa, pars = c(&#39;alpha&#39;,&#39;beta_phat&#39;,&#39;sigma_mu&#39;))</code></pre>
<pre><code>## Inference for Stan model: pass-prob-stan-model-no-epa.
## 4 chains, each with iter=1000; warmup=500; thin=1; 
## post-warmup draws per chain=500, total post-warmup draws=2000.
## 
##            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## alpha     -0.03       0 0.02 -0.07 -0.05 -0.03 -0.02  0.00   425 1.00
## beta_phat  1.19       0 0.01  1.17  1.18  1.19  1.19  1.20  3526 1.00
## sigma_mu   0.18       0 0.01  0.16  0.18  0.18  0.19  0.21   599 1.01
## 
## Samples were drawn using NUTS(diag_e) at Sun Aug 16 20:23:53 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<div id="team-effects" class="section level3 tabset">
<h3>Team Effects</h3>
<p>We can extract the samples from our model and use them to get our mean parameter estimates as well as the uncertainty in those estimates.</p>
<pre class="r"><code>samps_no_epa &lt;- rstan::extract(fit_no_epa, pars = &#39;mu&#39;)$mu
quantile_025_no_epa &lt;- apply(samps_no_epa, 2, quantile, .025)
quantile_975_no_epa &lt;- apply(samps_no_epa, 2, quantile, .975)
mean_no_epa &lt;- apply(samps_no_epa, 2, mean)

teams &lt;- dat %&gt;%
  filter(season &gt;= 2016 &amp; !is.na(posteam)) %&gt;%
  dplyr::select(posteam, season, qb_dropback) %&gt;%
  mutate(team_string = str_c(posteam, &#39;-&#39;, season),
         team_idx = as.numeric(factor(team_string))) %&gt;%
  group_by(posteam, season) %&gt;%
  summarise(team_idx = max(team_idx),
            dropback_pct = mean(qb_dropback)) %&gt;%
  ungroup()
  
teams$q_025_no_epa &lt;- quantile_025_no_epa
teams$q_975_no_epa &lt;- quantile_975_no_epa
teams$mean_no_epa &lt;- mean_no_epa
teams$display_name &lt;- factor(str_c(teams$posteam, &#39; - &#39;, teams$season))
teams$display_name &lt;- fct_reorder(teams$display_name, teams$mean_no_epa)</code></pre>
<p>The plots below show the estimated team effects. Note that the effects on the x-axis are on the log-odds scale. The 2018 Seahawks estimate of -.47 means that we would predict a Seahawks pass with probability .38 in a situation where the league-wide probability is .5. We would predict the 2018 Steelers to pass with probability .62 in that same situation.</p>
<p>One interesting thing is that, beyond 2018, the Seahawks haven’t been that big of an outlier. They were among the pass-heavier teams in 2016-17 and only slightly below average in 2019. We also see that some teams who run the ball a lot like the Patriots, Rams, and Saints show up as being more aggressive than dropback% would lead us to believe.</p>
<div id="section" class="section level4">
<h4>2016</h4>
<p><img src="/post/2020-09-02-measuring-run-pass-tendencies-with-tidymodels-and-nflfastr_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
<div id="section-1" class="section level4">
<h4>2017</h4>
<p><img src="/post/2020-09-02-measuring-run-pass-tendencies-with-tidymodels-and-nflfastr_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="section-2" class="section level4">
<h4>2018</h4>
<p><img src="/post/2020-09-02-measuring-run-pass-tendencies-with-tidymodels-and-nflfastr_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
</div>
<div id="section-3" class="section level4">
<h4>2019</h4>
<p><img src="/post/2020-09-02-measuring-run-pass-tendencies-with-tidymodels-and-nflfastr_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<p>The last thing I’ll show is how my estimated pass-heaviness correlates with QB Dropback%. To make the plot below I converted the model estimates and the actual QB dropback% into within-season ranks. Teams above the line are pass-heavier than their unadjusted dropback% would lead us to believe. Teams below the line are run-heavier. I highlight the Patriots to come back to the point at the beginning of the post. The Patriots consistently run more than average but are among the pass-heavier teams once game script is accounted for.</p>
<p><img src="/post/2020-09-02-measuring-run-pass-tendencies-with-tidymodels-and-nflfastr_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="conclusions" class="section level2">
<h2>Conclusions</h2>
<p>We showed here that we can use NFL play-by-play data to measure game script and better understand team tendencies. After adjusting for game script we find that teams that run the ball the most are not necessarily the run-heaviest teams. Except the 2018 Seahawks. They were the run heaviest team.</p>
<p>To go back to the Seahawks, this doesn’t really address the “Let Russ Cook” debate. The 2019 Seahawks weren’t overly run-heavy when game script is taken into account but a big part of this debate is that the Seahawks have a great quarterback which should probably influence how much they use him! In a future post I’ll build QB ability into our model which will give us a better idea of how big an outlier the Seahawks are.</p>
</div>
